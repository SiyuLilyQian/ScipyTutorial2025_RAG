{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Scipy Tutorial 2025 RAG**"
      ],
      "metadata": {
        "id": "Y-6UN66XeYTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1: LLM Inference Setup**\n",
        "---\n",
        "Before we explore the power of Retrieval-Augmented Generation, let’s first set up our LLM inference endpoint. For this tutorial, we’ll be using an open-source LLM.\n"
      ],
      "metadata": {
        "id": "6oa_lNh-e3vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Required Packages**"
      ],
      "metadata": {
        "id": "_Vrcl8-bEgqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate huggingface-hub langchain_huggingface langchain_community faiss-cpu"
      ],
      "metadata": {
        "id": "TKujLQRLC9k9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Launch a GPU instance**\n",
        "\n",
        "\n",
        "**Google Colab**: Open Google Colab, in setting Change Runtime Type, choose Runtime → Change Runtime Type to High Ram and pick a GPU.\n",
        "\n",
        "**Nebari**: If you’re using the Nebari platform, be sure to select a GPU instance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1040tOqkrj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences: CPU vs. GPU\n",
        "\n",
        "| Aspect            | CPU                                                         | GPU                                                      |\n",
        "|-------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
        "| **Function**      | Generalized component that handles main processing functions of a server | Specialized component that excels at parallel computing   |\n",
        "| **Processing**    | Designed for serial instruction processing                  | Designed for parallel instruction processing             |\n",
        "| **Design**        | Fewer, more powerful cores                                  | More cores than CPUs, but less powerful than CPU cores   |\n",
        "| **Best suited for** | General purpose computing applications                    | High-performance computing applications                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "sHWJRzAPnwNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Setup HuggingFace Token (Optional)**"
      ],
      "metadata": {
        "id": "hVSO59bYC0UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access tokens are required for gated or private models and are recommended for non-gated models.\n",
        "\n",
        "1.   Go to your Hugging Face account’s [Settings](https://huggingface.co/settings/tokens) → Access Tokens (huggingface.co/settings/tokens).\n",
        "2.   Click “New token”, give it a name, and select the “Read” scope (sufficient for this tutorial).\n",
        "3. Copy the generated token and save it in your Colab notebook as a secret in the Secrets section."
      ],
      "metadata": {
        "id": "V2I-cRraLQXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_Token\")\n",
        "login(token=os.environ[\"HF_TOKEN\"], new_session=False)"
      ],
      "metadata": {
        "id": "WDm5bTlFNlbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Instantiating a Text-Generation Pipeline with a Chat-Style Prompt**"
      ],
      "metadata": {
        "id": "utVilA0aM4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RIDd6M-OXWEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipelines are a great and easy way to use models for inference,offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering."
      ],
      "metadata": {
        "id": "Cxi7mIE7x4jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,\n",
        "    return_full_text=False, # don't return the prompt itself\n",
        ")"
      ],
      "metadata": {
        "id": "0BeQX-7inNaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate the LLM inference workflow into a minimal helper function for prompt templates, allowing users to provide their own context."
      ],
      "metadata": {
        "id": "2ZGLakllGtAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_template(context: str, question: str):\n",
        "    \"\"\"\n",
        "    context: supporting document or knowledge snippet\n",
        "    question: user’s query\n",
        "    \"\"\"\n",
        "    # build a prompt that clearly separates context from the question\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert question-answering assistant in a RAG (Retrieval-Augmented Generation) system.\n",
        "    Use only the information in the CONTEXT to ANSWER the QUESTION.\n",
        "    CONTEXT:\n",
        "    {context.strip()}\n",
        "    QUESTION:\n",
        "    {question.strip()}\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "    out = pipe(prompt, max_new_tokens=100, truncation=True, do_sample=True)[0]\n",
        "    return out[\"generated_text\"]"
      ],
      "metadata": {
        "id": "1T1VflQ2nnCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without Context**\n",
        "\n",
        "Without a defined knowledge context, the LLM may hallucinate and provide inaccurate information."
      ],
      "metadata": {
        "id": "i3Z6gkhmwnXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the canvas dimensions of “Les Demoiselles d’Avignon,” and what subject does the painting depict?\"\n",
        "prompt_template(\"\",user_question)"
      ],
      "metadata": {
        "id": "LD07gl1awrH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Context**\n",
        "\n",
        "With a clearly defined, fact-based context, the LLM can answer this question precisely."
      ],
      "metadata": {
        "id": "BRMIYVkvv92V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_input = \"\"\"\n",
        "In July 1907, Pablo Picasso unveiled “Les Demoiselles d’Avignon” in his Paris studio.\n",
        "This groundbreaking canvas (243 cm × 233 cm) depicts five nude female figures with angular,\n",
        "fragmented forms and faces inspired by African and Iberian masks.\n",
        "By abandoning traditional single-point perspective, Picasso flattened the pictorial space\n",
        "and presented multiple viewpoints simultaneously.\n",
        "The painting’s radical departure from realistic representation laid the groundwork for the\n",
        "Cubist movement, which Picasso and Georges Braque would develop further in 1908–1914.\n",
        "\"\"\"\n",
        "user_question = \"What are the canvas dimensions of “Les Demoiselles d’Avignon,” and what subject does the painting depict?\"\n",
        "prompt_template(context_input,user_question)"
      ],
      "metadata": {
        "id": "JBfO1Ukdn44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2: Load Data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1hxZGS5CqkUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the downloaded data by running `unzip scientific_papers.zip` in your terminal."
      ],
      "metadata": {
        "id": "6oMJHt1L2ySx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: On Google Colab, make sure you’ve mounted your Drive."
      ],
      "metadata": {
        "id": "V--JpYeyvs6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5WC4EZ6ky2oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify your Google Drive path (or file path on Nebari), then load the data"
      ],
      "metadata": {
        "id": "jxNDUBuw35t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = # file path"
      ],
      "metadata": {
        "id": "rAvDEYYu35E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
        "# Read them into a dict, keep track of file names\n",
        "documents_dict = {}\n",
        "for fp in txt_files:\n",
        "    with open(fp, 'r', encoding='utf-8') as f:\n",
        "        documents_dict[os.path.basename(fp)] = f.read()"
      ],
      "metadata": {
        "id": "aeWeQrpE39-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "# Convert each entry in documents_dict into a Document object\n",
        "docs = [\n",
        "    Document(page_content=content,metadata={\"source\": filename})\n",
        "    for filename, content in documents_dict.items()\n",
        "]\n",
        "print(f\"Number of documents loaded: {len(docs)}\")"
      ],
      "metadata": {
        "id": "14qSzmnF0-a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 3: RAG**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_jyAoq3PwD54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1 Chunking**\n",
        "\n",
        "Chunking refers to the process of splitting a larger document into smaller, more manageable “chunks” of text before embedding and retrieval.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p3onorJ442oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(separator=\"\", chunk_size=2000,chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "print(f\"{len(texts)} of chunks are created.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rezzyGs442C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What observations did you make about fixed-length chunking, and which alternative chunking method would you like to explore next?"
      ],
      "metadata": {
        "id": "vRAJLnvqspyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Here"
      ],
      "metadata": {
        "id": "APJL2a7ktAzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Measure each chunking strategy's processing latency. Which method runs the fastest, and which one is the slowest? Why is that?"
      ],
      "metadata": {
        "id": "_65qkm-HtVWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Here"
      ],
      "metadata": {
        "id": "f6s7Y4XLtqgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2 Embedding**\n",
        "\n",
        "\n",
        "Embedding and indexing are the steps that turn text chunks into a searchable vector database. **Embedding** converts pieces of text into high-dimensional numeric vectors that capture their semantic meaning.\n",
        "**Indexing** stores those vectors in a specialized data structure—or “index”—that supports fast similarity search."
      ],
      "metadata": {
        "id": "HDvrfllI4eGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to explore the wide range of embedding models available on Hugging Face.\n"
      ],
      "metadata": {
        "id": "zQ8qZLSp6BmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "mEoD8eBf1pBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(texts, hf)"
      ],
      "metadata": {
        "id": "xRpYj9Se3wln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 Retrieval**\n",
        "\n",
        "Retrieval refers to the process of finding and fetching the most relevant chunks (pieces of text) from your indexed knowledge base to serve as context for your LLM."
      ],
      "metadata": {
        "id": "zKlW4eli4thB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunks = vectorstore.similarity_search(\"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\",k=2)"
      ],
      "metadata": {
        "id": "nScHuczy4jw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check source document\n",
        "retrieved_chunks[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_t8ImFovUIi",
        "outputId": "0a7e7296-cbbb-4590-bd5f-d0619a25ddec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '2311.06428v2.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What code changes are needed to add both a similarity-score threshold and metadata-based filtering on top of your standard “top­k chunk” retriever in a RAG pipeline?"
      ],
      "metadata": {
        "id": "uJNQ0nP4rFNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### enter code here"
      ],
      "metadata": {
        "id": "RG8y5JT0rGFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 2:** What steps are required to plug a sparse retriever into your RAG workflow, replacing the default dense retriever?"
      ],
      "metadata": {
        "id": "rcO5dPD8qPzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "### enter code here"
      ],
      "metadata": {
        "id": "-LcoVBA-qPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Is there a quick way to evaluate your retrieval results? Hint: Use Metadata"
      ],
      "metadata": {
        "id": "yXAzR8P76gi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## enter code here"
      ],
      "metadata": {
        "id": "fpUOyrUd6uKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4 Gradio App**\n",
        "\n",
        "\n",
        "A Gradio app is a Python-powered interface that lets users interactively demo and test models through customizable input and output components.\n",
        "\n",
        "With your RAG pipeline in place, you’re all set to start chatting with your LLM-powered assistant!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5l3HIFuJrXNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(question):\n",
        "  ####swap your retriever here #####\n",
        "  chunks = vectorstore.similarity_search(question,k=2)\n",
        "  # put the retreived chunks into a context block\n",
        "  joined_chunks = [\"\".join(chunk.page_content) for chunk in chunks]\n",
        "  # reformat them into one Markdown block\n",
        "  context = \"\\n\\n---\\n\\n\".join(joined_chunks)\n",
        "  return context"
      ],
      "metadata": {
        "id": "BfPyZoma0owJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def rag_chat(question: str):\n",
        "    # 1) get context\n",
        "    context = retrieve(question)\n",
        "    # 2) generate answer\n",
        "    answer = prompt_template(context,question)\n",
        "    # return both to the UI\n",
        "    return context, answer\n",
        "# ── 3) Build and launch the app ──\n",
        "iface = gr.Interface(\n",
        "    fn=rag_chat,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask anything…\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Retrieved Context\"),\n",
        "        gr.Textbox(label=\"Answer\")\n",
        "    ],\n",
        "    title=\"Simple RAG Demo\",\n",
        "    description=\"Enter a question, see the retrieved context, and the LLM's answer.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ],
      "metadata": {
        "id": "LMNuoacsuhI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.5 Advance Section**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTo7KT-Qznzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5.1 Hybrid Retrieval**\n",
        "\n",
        "Hybrid retrieval combines traditional keyword-based search (e.g., BM25) with vector-based semantic search to surface results that are both lexically and conceptually relevant."
      ],
      "metadata": {
        "id": "AR-p6wFXzxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "## code here"
      ],
      "metadata": {
        "id": "VookXs5qzzrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Which combination method does this hybrid retriever use?"
      ],
      "metadata": {
        "id": "_MdS_W1Az91R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5.2 Cross-Encoder Reranker**\n",
        "\n",
        "A reranker is a secondary model that takes the top-N candidates from an initial retrieval stage and assigns them more precise relevance scores to produce a refined ranking.\n",
        "\n",
        "In this section, we’ve provided the code for a cross-encoder reranker. Feel free to explore it and try out different models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e3pApnkB1AsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# load reranker model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
        "model = model.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "def cross_encoder_rerank(question: str, doc: str) -> float:\n",
        "    pairs = [[question, doc]]\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(\n",
        "            pairs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "        ).to(model.device)\n",
        "        scores = model(**inputs).logits.view(-1).float()\n",
        "    return scores.item()\n"
      ],
      "metadata": {
        "id": "v4h84Opl1VMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Plug the reranker into your current RAG pipeline. Is the reranker’s result better than the initial retrieval result?"
      ],
      "metadata": {
        "id": "M578Y3vA2ByK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## code here"
      ],
      "metadata": {
        "id": "fIhWSlX66bTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}