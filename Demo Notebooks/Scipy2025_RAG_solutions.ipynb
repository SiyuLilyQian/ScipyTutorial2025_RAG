{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-6UN66XeYTL"
   },
   "source": [
    "# **Scipy Tutorial 2025 RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oa_lNh-e3vN"
   },
   "source": [
    "# **PART 1: LLM Inference Setup**\n",
    "---\n",
    "Before we explore the power of Retrieval-Augmented Generation, letâ€™s first set up our LLM inference endpoint. For this tutorial, weâ€™ll be using an open-source LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1040tOqkrj9"
   },
   "source": [
    "**Step 1: Launch a GPU instance**\n",
    "\n",
    "**Nebari**: If youâ€™re using the Nebari platform, be sure to select a GPU instance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHWJRzAPnwNo"
   },
   "source": [
    "Differences: CPU vs. GPU\n",
    "\n",
    "| Aspect            | CPU                                                         | GPU                                                      |\n",
    "|-------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Function**      | Generalized component that handles main processing functions of a server | Specialized component that excels at parallel computing   |\n",
    "| **Processing**    | Designed for serial instruction processing                  | Designed for parallel instruction processing             |\n",
    "| **Design**        | Fewer, more powerful cores                                  | More cores than CPUs, but less powerful than CPU cores   |\n",
    "| **Best suited for** | General purpose computing applications                    | High-performance computing applications                  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utVilA0aM4sg"
   },
   "source": [
    "**Step 2: Instantiating a Text-Generation Pipeline with a Chat-Style Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RIDd6M-OXWEs",
    "outputId": "265f3ad1-20b8-4a89-e81c-5772f2796f3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/scipy/f2963877-1751654305-32-rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cxi7mIE7x4jr"
   },
   "source": [
    "The pipelines are a great and easy way to use models for inference,offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0BeQX-7inNaN",
    "outputId": "a4b0aaee-9701-45d2-fe02-3b675faf60ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.1,\n",
    "    return_full_text=False, # don't return the prompt itself\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZGLakllGtAL"
   },
   "source": [
    "Integrate the LLM inference workflow into a minimal helper function for prompt templates, allowing users to provide their own context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1T1VflQ2nnCt"
   },
   "outputs": [],
   "source": [
    "def prompt_template(context: str, question: str):\n",
    "    \"\"\"\n",
    "    context: supporting document or knowledge snippet\n",
    "    question: userâ€™s query\n",
    "    \"\"\"\n",
    "    # build a prompt that clearly separates context from the question\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert question-answering assistant in a RAG (Retrieval-Augmented Generation) system.\n",
    "    Use only the information in the CONTEXT to ANSWER the QUESTION.\n",
    "    CONTEXT:\n",
    "    {context.strip()}\n",
    "    QUESTION:\n",
    "    {question.strip()}\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    out = pipe(prompt, max_new_tokens=100, truncation=True, do_sample=True)[0]\n",
    "    return out[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Z6gkhmwnXv"
   },
   "source": [
    "**Without Context**\n",
    "\n",
    "Without a defined knowledge context, the LLM may hallucinate and provide inaccurate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LD07gl1awrH9",
    "outputId": "cb7d9a4e-8424-413a-9492-bc0147388e1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe painting \"Les Demoiselles dâ€™Avignon\" by Pablo Picasso has a canvas dimension of 73 x 53 centimeters. The subject of the painting is a group of prostitutes, often referred to as \"the dancers\" or \"the courtesans,\" depicted in a raw and primitive style, marking a significant departure from traditional portraiture. This work is considered a precursor to Cubism and'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"What are the canvas dimensions of â€œLes Demoiselles dâ€™Avignon,â€ and what subject does the painting depict?\"\n",
    "prompt_template(\"\",user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRMIYVkvv92V"
   },
   "source": [
    "**With Context**\n",
    "\n",
    "With a clearly defined, fact-based context, the LLM can answer this question precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JBfO1Ukdn44Z",
    "outputId": "92e06e47-b31d-41eb-878c-3072ee562494"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The canvas dimensions of â€œLes Demoiselles dâ€™Avignonâ€ are 243 cm (width) Ã— 233 cm (height). The painting depicts five nude female figures.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_input = \"\"\"\n",
    "In July 1907, Pablo Picasso unveiled â€œLes Demoiselles dâ€™Avignonâ€ in his Paris studio.\n",
    "This groundbreaking canvas (243 cm Ã— 233 cm) depicts five nude female figures with angular,\n",
    "fragmented forms and faces inspired by African and Iberian masks.\n",
    "By abandoning traditional single-point perspective, Picasso flattened the pictorial space\n",
    "and presented multiple viewpoints simultaneously.\n",
    "The paintingâ€™s radical departure from realistic representation laid the groundwork for the\n",
    "Cubist movement, which Picasso and Georges Braque would develop further in 1908â€“1914.\n",
    "\"\"\"\n",
    "user_question = \"What are the canvas dimensions of â€œLes Demoiselles dâ€™Avignon,â€ and what subject does the painting depict?\"\n",
    "prompt_template(context_input,user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hxZGS5CqkUp"
   },
   "source": [
    "# **PART 2: Load Data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oMJHt1L2ySx"
   },
   "source": [
    "In this tutorial, weâ€™ll use 100 scientific papers as our knowledge base. These are real arXiv papers from computer science and AI research, forming a subset of the [SPIQA](https://huggingface.co/datasets/google/spiqa) dataset.\n",
    "Navigate to the Data folder in your terminal using cd Data. Then, unzip the downloaded file by running `unzip scientific_papers.zip` in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lhPrWVjgKTYy",
    "outputId": "20bcbc00-15ba-407b-f332-f66d3f602e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent path: /home/siyulilyqian@gmail.com\n",
      "file path: /home/siyulilyqian@gmail.com/ScipyTutorial2025_RAG/Data/scientific_papers\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# find parent path\n",
    "current_path = Path.cwd()\n",
    "root_path = current_path.parents[1]\n",
    "print(\"parent path:\", root_path)\n",
    "# specif data file path\n",
    "folder_path = root_path/\"ScipyTutorial2025_RAG/Data/scientific_papers\"\n",
    "print(\"file path:\",folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aeWeQrpE39-h"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "# Read them into a dict, keep track of file names\n",
    "documents_dict = {}\n",
    "for fp in txt_files:\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        documents_dict[os.path.basename(fp)] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "14qSzmnF0-a1",
    "outputId": "23c5b8c3-cc59-418b-896d-7a4dc3b62366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 100\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "# Convert each entry in documents_dict into a Document object\n",
    "docs = [\n",
    "    Document(page_content=content,metadata={\"source\": filename})\n",
    "    for filename, content in documents_dict.items()\n",
    "]\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jyAoq3PwD54"
   },
   "source": [
    "# **PART 3: RAG**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3onorJ442oz"
   },
   "source": [
    "# **3.1 Chunking**\n",
    "\n",
    "Chunking refers to the process of splitting a larger document into smaller, more manageable â€œchunksâ€ of text before embedding and retrieval.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rezzyGs442C1",
    "outputId": "d18d0148-2d0b-443a-ff9b-662187e8cc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004 of chunks are created.\n",
      "CPU times: user 7.85 s, sys: 0 ns, total: 7.85 s\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\", chunk_size=2000,chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)} of chunks are created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRAJLnvqspyL"
   },
   "source": [
    "**Question 1:** What observations did you make about fixed-length chunking, and which alternative chunking method would you like to explore next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "APJL2a7ktAzI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2678 of chunks are created.\n",
      "CPU times: user 92.6 ms, sys: 2.98 ms, total: 95.6 ms\n",
      "Wall time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Text-structured based:\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)} of chunks are created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In contextual bandits, the objective is to select an action Að´A, guided by contextual information Xð‘‹X, to maximize the resulting outcome Yð‘ŒY. This paradigm is prevalent in many real-world applications such as healthcare, personalized recommendation systems, or online advertising [1, 2, 3]. The objective is to perform actions, such as prescribing medication or recommending items, which lead to desired outcomes like improved patient health or higher click-through rates. Nonetheless, updating the policy presents challenges, as naÃ¯vely implementing a new, untested policy may raise ethical or financial concerns. For instance, prescribing a drug based on a new policy poses risks, as it may result in unexpected side effects. As a result, recent research [4, 5, 6, 7, 8, 9, 10, 11] has concentrated on evaluating the performance of new policies (target policy) using only existing data that was generated using the current policy (behaviour policy). This problem is known as Off-Policy Evaluation (OPE).\\n\\nCurrent OPE methods in contextual bandits, such as the Inverse Probability Weighting (IPW) [12] and Doubly Robust (DR) [13] estimators primarily account for the policy shift by re-weighting the data using the ratio of the target and behaviour polices to estimate the target policy value. This can be problematic as it may lead to high variance in the estimators in cases of substantial policy shifts. The issue is further exacerbated in situations with large action or context spaces [14], since in these cases the estimation of policy ratios is even more difficult leading to extreme bias and variance.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can check the chunk content here\n",
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018 of chunks are created.\n",
      "CPU times: user 4min 33s, sys: 1.05 s, total: 4min 34s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "text_splitter = SemanticChunker(\n",
    "    hf, breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)} of chunks are created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_65qkm-HtVWf"
   },
   "source": [
    "**Question 2:** Measure each chunking strategy's processing latency. Which method runs the fastest, and which one is the slowest? Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Chunking (which splits text into chunks based on semantic similarity) typically takes the longest processing time compared to other methods, due to the complexity of embeddings and computations involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDvrfllI4eGz"
   },
   "source": [
    "# **3.2 Embedding**\n",
    "\n",
    "\n",
    "Embedding and indexing are the steps that turn text chunks into a searchable vector database. **Embedding** converts pieces of text into high-dimensional numeric vectors that capture their semantic meaning.\n",
    "**Indexing** stores those vectors in a specialized data structureâ€”or â€œindexâ€â€”that supports fast similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ8qZLSp6BmF"
   },
   "source": [
    "Feel free to explore the wide range of embedding models available on Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xRpYj9Se3wln"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(texts, hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKlW4eli4thB"
   },
   "source": [
    "# **3.3 Retrieval**\n",
    "\n",
    "Retrieval refers to the process of finding and fetching the most relevant chunks (pieces of text) from your indexed knowledge base to serve as context for your LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nScHuczy4jw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 ms, sys: 92 Î¼s, total: 19.1 ms\n",
      "Wall time: 17.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "retrieved_chunks = vectorstore.similarity_search(\"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\",k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can initialize the retriever using a different method.\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    # optional parameters:\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "6_t8ImFovUIi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '2311.06428v2.txt'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check source document\n",
    "retrieved_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJNQ0nP4rFNg"
   },
   "source": [
    "**Question 1:** What code changes are needed to add both a similarity-score  and metadata-based filtering on top of your standard â€œtopÂ­k chunkâ€ retriever in a RAG pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "RG8y5JT0rGFo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 ms, sys: 1.02 ms, total: 19.6 ms\n",
      "Wall time: 18 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': '2311.06428v2.txt'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Use a metadata filter to search within a specific document or source.\n",
    "metadata_filter = {\"source\": \"2311.06428v2.txt\"}\n",
    "retrieved_chunks = vectorstore.similarity_search(\"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\",\\\n",
    "                                                 k=2, \\\n",
    "                                                 filter=metadata_filter)\n",
    "retrieved_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 2.06 ms, total: 19.4 ms\n",
      "Wall time: 17.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add a similarity-score threshold \n",
    "retrieved_chunks = vectorstore.similarity_search_with_relevance_scores(\"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\",\\\n",
    "                                                 k=2,\\\n",
    "                                                 score_treshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcO5dPD8qPzF"
   },
   "source": [
    "\n",
    "\n",
    "**Question 2:** What steps are required to plug a sparse retriever into your RAG workflow, replacing the default dense retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "-LcoVBA-qPPT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 ms, sys: 9.98 ms, total: 223 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "sparse_retriever = BM25Retriever.from_documents(docs,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 17 Î¼s, total: 1.36 ms\n",
      "Wall time: 1.34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\"\n",
    "retrieved_chunks = sparse_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.4 Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXAzR8P76gi7"
   },
   "source": [
    "**Question 3:** Is there a quick way to evaluate your retrieval results? Hint: Use Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "fpUOyrUd6uKV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path: /home/siyulilyqian@gmail.com/ScipyTutorial2025_RAG/Data/RAG_QA.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, let's run through the 100 questions at once\n",
    "# specify data file path\n",
    "import pandas as pd\n",
    "folder_path = root_path/\"ScipyTutorial2025_RAG/Data/RAG_QA.json\"\n",
    "print(\"file path:\",folder_path)\n",
    "qa_df = pd.read_json(folder_path)\n",
    "qa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 741 ms, sys: 3.88 ms, total: 745 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function to run retrieval and extract info\n",
    "def retrieve_info(question):\n",
    "    # Replace the retriever with the one you believe works best for this use case\n",
    "    docs = vectorstore.similarity_search(question, k=2)\n",
    "   \n",
    "    \n",
    "    # Grab text chunks and metadata from top k\n",
    "    retrieved_texts = [doc.page_content for doc in docs]\n",
    "    retrieved_sources = [doc.metadata.get(\"source\", \"\") for doc in docs]\n",
    "    \n",
    "    # Join texts to reformat them before sending them to the LLM\n",
    "    joined_texts = \"\\n\".join(retrieved_texts)\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"retrieved_texts\": joined_texts,\n",
    "        \"retrieved_sources\": retrieved_sources\n",
    "    })\n",
    "\n",
    "# Apply retrieval to all questions\n",
    "retrieval_results = qa_df[\"question\"].apply(retrieve_info)\n",
    "\n",
    "# Concatenate new columns to the original DataFrame\n",
    "qa_df_retrieved_result = pd.concat([qa_df, retrieval_results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Recall@2: 63.33%\n"
     ]
    }
   ],
   "source": [
    "# Check if ground truth source is in retrieved list\n",
    "qa_df_retrieved_result[\"correct_retrieval\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: row[\"source\"] in row[\"retrieved_sources\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute Recall@k\n",
    "retrieval_accuracy = qa_df_retrieved_result[\"correct_retrieval\"].mean()\n",
    "print(f\"Retriever Recall@2: {retrieval_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Response Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>retrieved_sources</th>\n",
       "      <th>correct_retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is One of the promises of AI?</td>\n",
       "      <td>One of the promises of AI is to enhance human ...</td>\n",
       "      <td>2311.01007v2.txt</td>\n",
       "      <td>This work builds on our previous work in [MSS2...</td>\n",
       "      <td>[2311.01007v2.txt, 2311.01007v2.txt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Sound source localization (SSL)?</td>\n",
       "      <td>Sound source localization (SSL) is the task of...</td>\n",
       "      <td>2311.01052v2.txt</td>\n",
       "      <td>Sound source localization (SSL) is the task of...</td>\n",
       "      <td>[2311.01052v2.txt, 2311.01052v2.txt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Markov Decision Process (MDP)?</td>\n",
       "      <td>Markov Decision Process (MDP) is defined by a ...</td>\n",
       "      <td>2311.01075v1.txt</td>\n",
       "      <td>Markov Decision Process (MDP) is defined by a ...</td>\n",
       "      <td>[2311.01075v1.txt, 2311.02194v1.txt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Considering the multimodal nature of t...</td>\n",
       "      <td>Considering the multimodal nature of the PNG t...</td>\n",
       "      <td>2311.01091v1.txt</td>\n",
       "      <td>Considering the multimodal nature of the PNG t...</td>\n",
       "      <td>[2311.01091v1.txt, 2311.13574v1.txt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is To enable a safer and more accurate sy...</td>\n",
       "      <td>To enable a safer and more accurate system, on...</td>\n",
       "      <td>2311.01106v1.txt</td>\n",
       "      <td>Classification:For classification problems, mi...</td>\n",
       "      <td>[2311.03570v1.txt, 2311.03570v1.txt]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                 What is One of the promises of AI?   \n",
       "1           What is Sound source localization (SSL)?   \n",
       "2             What is Markov Decision Process (MDP)?   \n",
       "3  What is Considering the multimodal nature of t...   \n",
       "4  What is To enable a safer and more accurate sy...   \n",
       "\n",
       "                                              answer            source  \\\n",
       "0  One of the promises of AI is to enhance human ...  2311.01007v2.txt   \n",
       "1  Sound source localization (SSL) is the task of...  2311.01052v2.txt   \n",
       "2  Markov Decision Process (MDP) is defined by a ...  2311.01075v1.txt   \n",
       "3  Considering the multimodal nature of the PNG t...  2311.01091v1.txt   \n",
       "4  To enable a safer and more accurate system, on...  2311.01106v1.txt   \n",
       "\n",
       "                                     retrieved_texts  \\\n",
       "0  This work builds on our previous work in [MSS2...   \n",
       "1  Sound source localization (SSL) is the task of...   \n",
       "2  Markov Decision Process (MDP) is defined by a ...   \n",
       "3  Considering the multimodal nature of the PNG t...   \n",
       "4  Classification:For classification problems, mi...   \n",
       "\n",
       "                      retrieved_sources  correct_retrieval  \n",
       "0  [2311.01007v2.txt, 2311.01007v2.txt]               True  \n",
       "1  [2311.01052v2.txt, 2311.01052v2.txt]               True  \n",
       "2  [2311.01075v1.txt, 2311.02194v1.txt]               True  \n",
       "3  [2311.01091v1.txt, 2311.13574v1.txt]               True  \n",
       "4  [2311.03570v1.txt, 2311.03570v1.txt]              False  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df_retrieved_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first ask LLM to generate response with the retrieved context\n",
    "qa_df_retrieved_result[\"llm_response\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: prompt_template(row[\"retrieved_texts\"], row[\"question\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def compute_cosine_similarity(text_a, text_b):\n",
    "    # Embed two texts and calculate the similarity score\n",
    "    emb_llm = model.encode(text_a, convert_to_tensor=True)\n",
    "    emb_ref = model.encode(text_b, convert_to_tensor=True)\n",
    "    score = util.cos_sim(emb_llm, emb_ref).item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_retrieved_result[\"response_score\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: compute_cosine_similarity(row[\"llm_response\"],row[\"answer\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_template(question: str, reference_answer: str, llm_answer: str):\n",
    "    # Use an LLM to evaluate how well the llm_answer matches the reference_answer\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert question-answering assistant.\n",
    "    \n",
    "    You are given a question and two answers:\n",
    "    Question:\n",
    "    {question}\n",
    "    Reference Answer:\n",
    "    {reference_answer}\n",
    "    LLM-Generated Answer:\n",
    "    {llm_answer}\n",
    "    \n",
    "    Evaluate how similar and correct the LLM's answer is compared to the reference answer.\n",
    "    Score it from 0 to 1.\n",
    "    Output the numeric score only.\n",
    "    \"\"\"\n",
    "    out = pipe(prompt, max_new_tokens=100, truncation=True, do_sample=True)[0]\n",
    "    return out[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 9s, sys: 4.71 s, total: 4min 14s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_df_retrieved_result[\"response_score_llm_judge\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: evaluation_template(row['question'],row[\"llm_response\"],row[\"answer\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l3HIFuJrXNS"
   },
   "source": [
    "# **3.4 Gradio App**\n",
    "\n",
    "\n",
    "A Gradio app is a Python-powered interface that lets users interactively demo and test models through customizable input and output components.\n",
    "\n",
    "With your RAG pipeline in place, youâ€™re all set to start chatting with your LLM-powered assistant!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfPyZoma0owJ"
   },
   "outputs": [],
   "source": [
    "def retrieve(question):\n",
    "  ####swap your retriever here #####\n",
    "  chunks = vectorstore.similarity_search(question,k=2)\n",
    "  # put the retreived chunks into a context block\n",
    "  joined_chunks = [\"\".join(chunk.page_content) for chunk in chunks]\n",
    "  # reformat them into one Markdown block\n",
    "  context = \"\\n\\n---\\n\\n\".join(joined_chunks)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMNuoacsuhI2"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def rag_chat(question: str):\n",
    "    # 1) get context\n",
    "    context = retrieve(question)\n",
    "    # 2) generate answer\n",
    "    answer = prompt_template(context,question)\n",
    "    # return both to the UI\n",
    "    return context, answer\n",
    "# â”€â”€ 3) Build and launch the app â”€â”€\n",
    "iface = gr.Interface(\n",
    "    fn=rag_chat,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask anythingâ€¦\"),\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Retrieved Context\"),\n",
    "        gr.Textbox(label=\"Answer\")\n",
    "    ],\n",
    "    title=\"Simple RAG Demo\",\n",
    "    description=\"Enter a question, see the retrieved context, and the LLM's answer.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTo7KT-Qznzv"
   },
   "source": [
    "# **3.5 Advance Section**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR-p6wFXzxIY"
   },
   "source": [
    "## **3.5.1 Hybrid Retrieval**\n",
    "\n",
    "Hybrid retrieval combines traditional keyword-based search (e.g., BM25) with vector-based semantic search to surface results that are both lexically and conceptually relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "VookXs5qzzrk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Recall@2: 78.33%\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_retriever,vector_retriever],\n",
    "    strategy=\"merge\",\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# Function to run retrieval and extract info\n",
    "def retrieve_info(question):\n",
    "    # Replace the retriever with the one you believe works best for this use case\n",
    "    docs =hybrid_retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Grab text chunks and metadata from top k\n",
    "    retrieved_texts = [doc.page_content for doc in docs]\n",
    "    retrieved_sources = [doc.metadata.get(\"source\", \"\") for doc in docs]\n",
    "    \n",
    "    # Join texts to reformat them before sending them to the LLM\n",
    "    joined_texts = \"\\n\".join(retrieved_texts)\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"retrieved_texts\": joined_texts,\n",
    "        \"retrieved_sources\": retrieved_sources\n",
    "    })\n",
    "\n",
    "# Apply retrieval to all questions\n",
    "retrieval_results = qa_df[\"question\"].apply(retrieve_info)\n",
    "\n",
    "# Concatenate new columns to the original DataFrame\n",
    "qa_df_retrieved_result = pd.concat([qa_df, retrieval_results], axis=1)\n",
    "# Check if ground truth source is in retrieved list\n",
    "qa_df_retrieved_result[\"correct_retrieval\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: row[\"source\"] in row[\"retrieved_sources\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute Recall@k\n",
    "retrieval_accuracy = qa_df_retrieved_result[\"correct_retrieval\"].mean()\n",
    "print(f\"Retriever Recall@2: {retrieval_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MdS_W1Az91R"
   },
   "source": [
    "**Question:** Which combination method does this hybrid retriever use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3pApnkB1AsJ"
   },
   "source": [
    "## **3.5.2 Cross-Encoder Reranker**\n",
    "\n",
    "A reranker is a secondary model that takes the top-N candidates from an initial retrieval stage and assigns them more precise relevance scores to produce a refined ranking.\n",
    "\n",
    "In this section, weâ€™ve provided the code for a cross-encoder reranker. Feel free to explore it and try out different models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "v4h84Opl1VMy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# load reranker model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "model = model.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "def cross_encoder_rerank(question: str, doc: str) -> float:\n",
    "    pairs = [[question, doc]]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            pairs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "        ).to(model.device)\n",
    "        scores = model(**inputs).logits.view(-1).float()\n",
    "    return scores.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M578Y3vA2ByK"
   },
   "source": [
    "**Question**: Plug the reranker into your current RAG pipeline. Is the rerankerâ€™s result better than the initial retrieval result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "fIhWSlX66bTN"
   },
   "outputs": [],
   "source": [
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_retriever,vector_retriever],\n",
    "    strategy=\"merge\",\n",
    "    k=10,\n",
    ")\n",
    "def retrieve_info(question: str, top_k: int = 2) -> pd.Series:\n",
    "    # 1. get an initial pool (we set hybrid k=10 above)\n",
    "    docs = hybrid_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 2. rerank each hit with your cross-encoder\n",
    "    scored = []\n",
    "    for doc in docs:\n",
    "        score = cross_encoder_rerank(question, doc.page_content)\n",
    "        scored.append((doc, score))\n",
    "\n",
    "    # 3. sort by descending rerank score and truncate to your final top_k\n",
    "    scored = sorted(scored, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    docs = [doc for doc, _ in scored]\n",
    "\n",
    "    # 4. extract for your downstream pipeline\n",
    "    retrieved_texts = [d.page_content for d in docs]\n",
    "    retrieved_sources = [d.metadata.get(\"source\",\"\") for d in docs]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"retrieved_texts\": \"\\n\".join(retrieved_texts),\n",
    "        \"retrieved_sources\": retrieved_sources\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply retrieval to all questions\n",
    "retrieval_results = qa_df[\"question\"].apply(retrieve_info)\n",
    "\n",
    "# Concatenate new columns to the original DataFrame\n",
    "qa_df_retrieved_result = pd.concat([qa_df, retrieval_results], axis=1)\n",
    "# Check if ground truth source is in retrieved list\n",
    "qa_df_retrieved_result[\"correct_retrieval\"] = qa_df_retrieved_result.apply(\n",
    "    lambda row: row[\"source\"] in row[\"retrieved_sources\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute Recall@k\n",
    "retrieval_accuracy = qa_df_retrieved_result[\"correct_retrieval\"].mean()\n",
    "print(f\"Retriever Recall@2: {retrieval_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "scipy-scipy-rag",
   "language": "python",
   "name": "conda-env-scipy-scipy-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
