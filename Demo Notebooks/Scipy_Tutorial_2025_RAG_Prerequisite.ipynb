{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scipy Tutorial 2025 RAG"
      ],
      "metadata": {
        "id": "Y-6UN66XeYTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Prerequisites: LLM Inference Setup**\n",
        "---\n",
        "Before we explore the power of Retrieval-Augmented Generation, let’s first set up our LLM inference endpoint.\n"
      ],
      "metadata": {
        "id": "6oa_lNh-e3vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "***Use Open Source LLM***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJtY0yeefZYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Required Packages**"
      ],
      "metadata": {
        "id": "_Vrcl8-bEgqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate huggingface-hub"
      ],
      "metadata": {
        "id": "TKujLQRLC9k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Up Google Colab**\n",
        "\n",
        "\n",
        "\n",
        "Open Google Colab, in setting Change Runtime Type, choose Runtime → Change Runtime Type to High Ram and pick a GPU\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1040tOqkrj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences: CPU vs. GPU\n",
        "\n",
        "| Aspect            | CPU                                                         | GPU                                                      |\n",
        "|-------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
        "| **Function**      | Generalized component that handles main processing functions of a server | Specialized component that excels at parallel computing   |\n",
        "| **Processing**    | Designed for serial instruction processing                  | Designed for parallel instruction processing             |\n",
        "| **Design**        | Fewer, more powerful cores                                  | More cores than CPUs, but less powerful than CPU cores   |\n",
        "| **Best suited for** | General purpose computing applications                    | High-performance computing applications                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "sHWJRzAPnwNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Memory"
      ],
      "metadata": {
        "id": "jNTsX5OWWQRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "YVhCUOVeWLRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU"
      ],
      "metadata": {
        "id": "Ez6bfxb0WVdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "LKWkUu-RWXqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Setup HuggingFace Token**"
      ],
      "metadata": {
        "id": "hVSO59bYC0UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Go to your Hugging Face account’s [Settings](https://huggingface.co/settings/tokens) → Access Tokens (huggingface.co/settings/tokens).\n",
        "2.   Click “New token”, give it a name, and select the “Read” scope (sufficient for this tutorial).\n",
        "3. Copy the generated token and save it in your Colab notebook as a secret in the Secrets section."
      ],
      "metadata": {
        "id": "V2I-cRraLQXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_Token\")\n",
        "login(token=os.environ[\"HF_TOKEN\"], new_session=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDm5bTlFNlbr",
        "outputId": "79fc482f-e8da-4ad6-a2d4-23215b14b91c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Instantiating a LLaMA-8B Text-Generation Pipeline with a Chat-Style Prompt**"
      ],
      "metadata": {
        "id": "utVilA0aM4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RIDd6M-OXWEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipelines are a great and easy way to use models for inference,offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering."
      ],
      "metadata": {
        "id": "Cxi7mIE7x4jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,\n",
        "    return_full_text=False, # don't return the prompt itself\n",
        "    device_map=\"auto\",        # spread layers across GPUs/CPU\n",
        ")"
      ],
      "metadata": {
        "id": "0BeQX-7inNaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate the LLM inference workflow into a minimal RAG helper function that lets users supply their own context."
      ],
      "metadata": {
        "id": "2ZGLakllGtAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_generate(context: str, question: str):\n",
        "    \"\"\"\n",
        "    context: supporting document or knowledge snippet\n",
        "    question: user’s query\n",
        "    \"\"\"\n",
        "    # build a prompt that clearly separates context from the question\n",
        "    prompt = (\n",
        "        f\"\"\"Context:\\n{context.strip()}\\n\\n\n",
        "        Question: {question.strip()}\\n\n",
        "        Answer:\"\"\"\n",
        "    )\n",
        "    out = pipe(prompt, max_new_tokens=500, truncation=True, do_sample=True)[0]\n",
        "    return out[\"generated_text\"]"
      ],
      "metadata": {
        "id": "1T1VflQ2nnCt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WITH Context**\n",
        "\n",
        "With a clearly defined, fact-based context, the LLM can answer this question precisely."
      ],
      "metadata": {
        "id": "BRMIYVkvv92V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_input = \"\"\"\n",
        "In July 1907, Pablo Picasso unveiled “Les Demoiselles d’Avignon” in his Paris studio.\n",
        "This groundbreaking canvas (243 cm × 233 cm) depicts five nude female figures with angular,\n",
        "fragmented forms and faces inspired by African and Iberian masks.\n",
        "By abandoning traditional single-point perspective, Picasso flattened the pictorial space\n",
        "and presented multiple viewpoints simultaneously.\n",
        "The painting’s radical departure from realistic representation laid the groundwork for the\n",
        "Cubist movement, which Picasso and Georges Braque would develop further in 1908–1914.\n",
        "\"\"\"\n",
        "user_question = \"What are the canvas dimensions of “Les Demoiselles d’Avignon,” and what subject does the painting depict?\"\n",
        "\n",
        "rag_generate(context_input,user_question)"
      ],
      "metadata": {
        "id": "JBfO1Ukdn44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WITHOUT Context**\n",
        "\n",
        "Without a defined knowledge context, the LLM may hallucinate and provide inaccurate information."
      ],
      "metadata": {
        "id": "i3Z6gkhmwnXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_generate(\"None\",user_question)"
      ],
      "metadata": {
        "id": "LD07gl1awrH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "https://aws.amazon.com/compare/the-difference-between-gpus-cpus/"
      ],
      "metadata": {
        "id": "KnC0bb0ilzGu"
      }
    }
  ]
}