{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "_UUuPxPyTkNx",
      "metadata": {
        "id": "_UUuPxPyTkNx"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install langchain_community\n",
        "!pip install pypdf\n",
        "!pip install langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oW6QzI_pLXbU",
      "metadata": {
        "id": "oW6QzI_pLXbU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "from langchain.schema import Document\n",
        "from Modules import scipy_chunking_helper as chunking_helper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae43fed-2bea-415f-b6b7-ac5e2bf85ce6",
      "metadata": {
        "id": "8ae43fed-2bea-415f-b6b7-ac5e2bf85ce6"
      },
      "source": [
        "# Chunking Methods for RAG Pipelines\n",
        "---\n",
        "This notebook demonstrates several methods to split text documents into useful chunks for retrieval-augmented generation (RAG) using LangChain and related libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5489fb2b-2965-4881-8c8b-bdd1623138ca",
      "metadata": {
        "id": "5489fb2b-2965-4881-8c8b-bdd1623138ca"
      },
      "source": [
        "## 1. Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "81a2e62e-6660-4fbc-a810-4541d46e4fd7",
      "metadata": {
        "id": "81a2e62e-6660-4fbc-a810-4541d46e4fd7"
      },
      "outputs": [],
      "source": [
        "sample_text = \"\"\"# Introduction\n",
        "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation.\n",
        "\n",
        "## What is Chunking?\n",
        "Chunking is the process of dividing text into manageable segments.\n",
        "- Character-based chunking splits purely by length.\n",
        "- Recursive chunking uses natural text breaks, like paragraphs or sentences.\n",
        "- Semantic chunking finds topic changes.\n",
        "\n",
        "Here is a list:\n",
        "1. Fast and easy: character-based.\n",
        "2. Natural: recursive or markdown-based.\n",
        "3. Context-aware: semantic.\n",
        "\n",
        "## An Example with a Superlongword\n",
        "Sometimes, data includes strange artifacts like:\n",
        "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
        "\n",
        "## Topic Change: Semantic Matters\n",
        "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
        "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
        "\n",
        "In summary, choose your chunker based on your data and task!\"\"\"\n",
        "doc = Document(page_content=sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144cc0ed-21eb-472a-8dfa-e308254f93f7",
      "metadata": {
        "id": "144cc0ed-21eb-472a-8dfa-e308254f93f7"
      },
      "source": [
        "## 2. Different Chunking Methods\n",
        "### 2.1 Character Splitter\n",
        "Splits text into fixed-length character chunks (with optional overlap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "vx7SmQxAUaAC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx7SmQxAUaAC",
        "outputId": "5d27d841-baf5-4d8b-956b-5d684c9c83e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 6\n"
          ]
        }
      ],
      "source": [
        "char_chunks = chunking_helper.fixed_length_chunking(doc, 200, 0, '')\n",
        "print(f\"Total chunks: {len(char_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "LcD1cG5snL76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcD1cG5snL76",
        "outputId": "aa71ae3a-5b3a-4de4-ad95-12768d60e34e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 (length: 200):\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and gene\n",
            "----------------------------------------\n",
            "Chunk 2 (length: 200):\n",
            "ration.\n",
            "\n",
            "## What is Chunking?\n",
            "Chunking is the process of dividing text into manageable segments.\n",
            "- Character-based chunking splits purely by length.\n",
            "- Recursive chunking uses natural text breaks, like\n",
            "----------------------------------------\n",
            "Chunk 3 (length: 199):\n",
            "paragraphs or sentences.\n",
            "- Semantic chunking finds topic changes.\n",
            "\n",
            "Here is a list:\n",
            "1. Fast and easy: character-based.\n",
            "2. Natural: recursive or markdown-based.\n",
            "3. Context-aware: semantic.\n",
            "\n",
            "## An Examp\n",
            "----------------------------------------\n",
            "Chunk 4 (length: 200):\n",
            "le with a Superlongword\n",
            "Sometimes, data includes strange artifacts like:\n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "\n",
            "## Top\n",
            "----------------------------------------\n",
            "Chunk 5 (length: 200):\n",
            "ic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
            "Semantic chunking is especially useful when there are\n",
            "----------------------------------------\n",
            "Chunk 6 (length: 148):\n",
            "clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
            "\n",
            "In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chunking_helper.print_chunks(char_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f550ff-fb3c-4e56-a45d-2a70e09561ca",
      "metadata": {
        "id": "90f550ff-fb3c-4e56-a45d-2a70e09561ca"
      },
      "source": [
        "### 2.2 Recursive Splitter\n",
        "Attempts to split text by different logical separators (e.g., paragraphs, sentences) for more \"natural\" chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Vm5rpGHKn6Mf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm5rpGHKn6Mf",
        "outputId": "a32a014d-11a8-4d38-fa15-f0378c9b9fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 10\n"
          ]
        }
      ],
      "source": [
        "recursive_chunks = chunking_helper.recursive_chunking(doc, 200, 0)\n",
        "print(f\"Total chunks: {len(recursive_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "96bf8ddd-df18-4e0a-9059-ba4af373a4ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96bf8ddd-df18-4e0a-9059-ba4af373a4ee",
        "outputId": "c8dc72e5-22ab-4edf-bc2b-bf975a0e790a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 (length: 14):\n",
            "# Introduction\n",
            "----------------------------------------\n",
            "Chunk 2 (length: 192):\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation.\n",
            "----------------------------------------\n",
            "Chunk 3 (length: 139):\n",
            "## What is Chunking?\n",
            "Chunking is the process of dividing text into manageable segments.\n",
            "- Character-based chunking splits purely by length.\n",
            "----------------------------------------\n",
            "Chunk 4 (length: 117):\n",
            "- Recursive chunking uses natural text breaks, like paragraphs or sentences.\n",
            "- Semantic chunking finds topic changes.\n",
            "----------------------------------------\n",
            "Chunk 5 (length: 119):\n",
            "Here is a list:\n",
            "1. Fast and easy: character-based.\n",
            "2. Natural: recursive or markdown-based.\n",
            "3. Context-aware: semantic.\n",
            "----------------------------------------\n",
            "Chunk 6 (length: 83):\n",
            "## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like:\n",
            "----------------------------------------\n",
            "Chunk 7 (length: 119):\n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "----------------------------------------\n",
            "Chunk 8 (length: 152):\n",
            "## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
            "----------------------------------------\n",
            "Chunk 9 (length: 140):\n",
            "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
            "----------------------------------------\n",
            "Chunk 10 (length: 60):\n",
            "In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chunking_helper.print_chunks(recursive_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64dfd0b0-c653-47f0-a0b3-2befefa99e92",
      "metadata": {
        "id": "64dfd0b0-c653-47f0-a0b3-2befefa99e92"
      },
      "source": [
        "### 2.3 Semantic Chunking\n",
        "Chunking based on **semantic similarity**â€”splitting where the topic shifts rather than at fixed lengths.  \n",
        "We'll use `sentence-transformers` to embed sentences and split at points with high semantic \"distance\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Gt_D3vz6FHMd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt_D3vz6FHMd",
        "outputId": "cff87c11-a86e-40e9-d9ae-235b614c52c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 6\n"
          ]
        }
      ],
      "source": [
        "semantic_chunks = chunking_helper.semantic_chunking(doc, \"sentence-transformers/all-mpnet-base-v2\", \"percentile\", 70)\n",
        "print(f\"Total chunks: {len(semantic_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "57ef175e-c4e1-4344-82f4-00b25631cb04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ef175e-c4e1-4344-82f4-00b25631cb04",
        "outputId": "6d88dbf0-27be-4b4f-f4db-d3d928e6e4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 (length: 465):\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation. ## What is Chunking? Chunking is the process of dividing text into manageable segments. - Character-based chunking splits purely by length. - Recursive chunking uses natural text breaks, like paragraphs or sentences. - Semantic chunking finds topic changes.\n",
            "----------------------------------------\n",
            "Chunk 2 (length: 18):\n",
            "Here is a list:\n",
            "1.\n",
            "----------------------------------------\n",
            "Chunk 3 (length: 31):\n",
            "Fast and easy: character-based.\n",
            "----------------------------------------\n",
            "Chunk 4 (length: 43):\n",
            "2. Natural: recursive or markdown-based. 3.\n",
            "----------------------------------------\n",
            "Chunk 5 (length: 24):\n",
            "Context-aware: semantic.\n",
            "----------------------------------------\n",
            "Chunk 6 (length: 558):\n",
            "## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like:\n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters. ## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter. Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading. In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chunking_helper.print_chunks(semantic_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MFfrPz4JV7Hk",
      "metadata": {
        "id": "MFfrPz4JV7Hk"
      },
      "source": [
        "### 2.4 Semantic Chunking with Chunk Size Controlled\n",
        "Using the same logic, but\n",
        "\n",
        "\n",
        "*   Merge small chunks to the previous one\n",
        "*   Set up the max chunk size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eCi60sgrG603",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCi60sgrG603",
        "outputId": "3eaa2e00-e745-44a1-ec34-b25576cd9421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 5\n"
          ]
        }
      ],
      "source": [
        "semantic_chunks_2 = chunking_helper.semantic_chunking_improved(doc, \"all-mpnet-base-v2\", \"percentile\", 70)\n",
        "print(f\"Total chunks: {len(semantic_chunks_2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ymP1YPjlJwJ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymP1YPjlJwJ0",
        "outputId": "7370932a-208c-4515-e7fc-1da26821f751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 (length: 52):\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking!\n",
            "----------------------------------------\n",
            "Chunk 2 (length: 532):\n",
            "This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation. ## What is Chunking? Chunking is the process of dividing text into manageable segments. - Character-based chunking splits purely by length. - Recursive chunking uses natural text breaks, like paragraphs or sentences. - Semantic chunking finds topic changes. Here is a list:\n",
            "1. Fast and easy: character-based. 2. Natural: recursive or markdown-based. 3. Context-aware: semantic.\n",
            "----------------------------------------\n",
            "Chunk 3 (length: 203):\n",
            "## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like:\n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "----------------------------------------\n",
            "Chunk 4 (length: 152):\n",
            "## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
            "----------------------------------------\n",
            "Chunk 5 (length: 201):\n",
            "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading. In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chunking_helper.print_chunks(semantic_chunks_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "anaconda-ai-2024.04-py310",
      "language": "python",
      "name": "conda-env-anaconda-ai-2024.04-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
