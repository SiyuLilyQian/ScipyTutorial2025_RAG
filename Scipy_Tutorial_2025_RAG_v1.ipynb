{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scipy Tutorial 2025 RAG"
      ],
      "metadata": {
        "id": "Y-6UN66XeYTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Prerequisites: LLM Inference Setup**\n",
        "---\n",
        "Before we explore the power of Retrieval-Augmented Generation, let’s first set up our LLM inference endpoint.\n"
      ],
      "metadata": {
        "id": "6oa_lNh-e3vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "***Use Open Source LLM***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJtY0yeefZYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Required Packages**\n",
        "\n",
        "Both for LLM and later RAG portion"
      ],
      "metadata": {
        "id": "_Vrcl8-bEgqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate huggingface-hub langchain_huggingface langchain_community faiss-cpu"
      ],
      "metadata": {
        "id": "TKujLQRLC9k9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Up Google Colab**\n",
        "\n",
        "\n",
        "\n",
        "Open Google Colab, in setting Change Runtime Type, choose Runtime → Change Runtime Type to High Ram and pick a GPU.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1040tOqkrj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences: CPU vs. GPU\n",
        "\n",
        "| Aspect            | CPU                                                         | GPU                                                      |\n",
        "|-------------------|-------------------------------------------------------------|----------------------------------------------------------|\n",
        "| **Function**      | Generalized component that handles main processing functions of a server | Specialized component that excels at parallel computing   |\n",
        "| **Processing**    | Designed for serial instruction processing                  | Designed for parallel instruction processing             |\n",
        "| **Design**        | Fewer, more powerful cores                                  | More cores than CPUs, but less powerful than CPU cores   |\n",
        "| **Best suited for** | General purpose computing applications                    | High-performance computing applications                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "sHWJRzAPnwNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Setup HuggingFace Token**"
      ],
      "metadata": {
        "id": "hVSO59bYC0UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Go to your Hugging Face account’s [Settings](https://huggingface.co/settings/tokens) → Access Tokens (huggingface.co/settings/tokens).\n",
        "2.   Click “New token”, give it a name, and select the “Read” scope (sufficient for this tutorial).\n",
        "3. Copy the generated token and save it in your Colab notebook as a secret in the Secrets section."
      ],
      "metadata": {
        "id": "V2I-cRraLQXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_Token\")\n",
        "login(token=os.environ[\"HF_TOKEN\"], new_session=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDm5bTlFNlbr",
        "outputId": "df6217fd-cc77-40f0-acc8-7ffd9641b5bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Instantiating a Text-Generation Pipeline with a Chat-Style Prompt**"
      ],
      "metadata": {
        "id": "utVilA0aM4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.1-2b-instruct\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RIDd6M-OXWEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipelines are a great and easy way to use models for inference,offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering."
      ],
      "metadata": {
        "id": "Cxi7mIE7x4jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,\n",
        "    return_full_text=False, # don't return the prompt itself\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BeQX-7inNaN",
        "outputId": "3a69f020-84c2-48b4-f1df-a4ad2c67f7b8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate the LLM inference workflow into a minimal RAG helper function that lets users supply their own context."
      ],
      "metadata": {
        "id": "2ZGLakllGtAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_generate(context: str, question: str):\n",
        "    \"\"\"\n",
        "    context: supporting document or knowledge snippet\n",
        "    question: user’s query\n",
        "    \"\"\"\n",
        "    # build a prompt that clearly separates context from the question\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert question-answering assistant in a RAG (Retrieval-Augmented Generation) system.\n",
        "    Use only the information in the CONTEXT to ANSWER the QUESTION.\n",
        "    CONTEXT:\n",
        "    {context.strip()}\n",
        "    QUESTION:\n",
        "    {question.strip()}\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "    out = pipe(prompt, max_new_tokens=100, truncation=True, do_sample=True)[0]\n",
        "    return out[\"generated_text\"]"
      ],
      "metadata": {
        "id": "1T1VflQ2nnCt"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WITH Context**\n",
        "\n",
        "With a clearly defined, fact-based context, the LLM can answer this question precisely."
      ],
      "metadata": {
        "id": "BRMIYVkvv92V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_input = \"\"\"\n",
        "In July 1907, Pablo Picasso unveiled “Les Demoiselles d’Avignon” in his Paris studio.\n",
        "This groundbreaking canvas (243 cm × 233 cm) depicts five nude female figures with angular,\n",
        "fragmented forms and faces inspired by African and Iberian masks.\n",
        "By abandoning traditional single-point perspective, Picasso flattened the pictorial space\n",
        "and presented multiple viewpoints simultaneously.\n",
        "The painting’s radical departure from realistic representation laid the groundwork for the\n",
        "Cubist movement, which Picasso and Georges Braque would develop further in 1908–1914.\n",
        "\"\"\"\n",
        "user_question = \"What are the canvas dimensions of “Les Demoiselles d’Avignon,” and what subject does the painting depict?\"\n",
        "\n",
        "rag_generate(context_input,user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JBfO1Ukdn44Z",
        "outputId": "ca39cb87-fe13-4c98-eb38-5de593da635d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The canvas dimensions of “Les Demoiselles d’Avignon” are 243 cm (width) × 233 cm (height). The painting depicts five nude female figures.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WITHOUT Context**\n",
        "\n",
        "Without a defined knowledge context, the LLM may hallucinate and provide inaccurate information."
      ],
      "metadata": {
        "id": "i3Z6gkhmwnXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_generate(\"\",user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "LD07gl1awrH9",
        "outputId": "7ca3dd87-37f4-4c82-fc5e-f2e40ac1faa2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe painting \"Les Demoiselles d’Avignon\" by Pablo Picasso has canvas dimensions of 73 x 53 centimeters. The subject of the painting is a group of prostitutes, often referred to as \"The Bather\" and \"The Nude,\" depicted in a raw and primitive style, marking a significant departure from traditional European art. This work is considered a precursor to Cubism and is renown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Load Data**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4IVVmjs7ynhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WC4EZ6ky2oR",
        "outputId": "27bd3276-747c-4690-a32a-4c7522ad4d59"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "# Find all .txt files in that folder\n",
        "folder_path = '/content/drive/MyDrive/Scipy_Data/selected_files_scipy'\n",
        "txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
        "# Read them into a dict, keep track of file names\n",
        "documents_dict = {}\n",
        "for fp in txt_files:\n",
        "    with open(fp, 'r', encoding='utf-8') as f:\n",
        "        documents_dict[os.path.basename(fp)] = f.read()"
      ],
      "metadata": {
        "id": "r3CbH8ioy5Q1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "docs = [\n",
        "    Document(page_content=content,metadata={\"source\": filename})\n",
        "    for filename, content in documents_dict.items()\n",
        "]\n",
        "print(f\"Number of documents loaded: {len(docs)}\")"
      ],
      "metadata": {
        "id": "14qSzmnF0-a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca759ac-49fd-41a3-8530-08549c5cbc7f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents loaded: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Chunking**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p3onorJ442oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "print(f\"{len(texts)} of chunks are created.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rezzyGs442C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What observations did you make about fixed-length chunking, and which alternative chunking method would you like to explore next?"
      ],
      "metadata": {
        "id": "vRAJLnvqspyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Here"
      ],
      "metadata": {
        "id": "APJL2a7ktAzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Measure each chunking strategy's processing latency. Which method runs the fastest, and which one is the slowest? Why is that?"
      ],
      "metadata": {
        "id": "_65qkm-HtVWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Here"
      ],
      "metadata": {
        "id": "f6s7Y4XLtqgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Indexing**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HDvrfllI4eGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to explore the wide range of embedding models available on Hugging Face.\n"
      ],
      "metadata": {
        "id": "zQ8qZLSp6BmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "mEoD8eBf1pBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(texts, hf)"
      ],
      "metadata": {
        "id": "xRpYj9Se3wln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Retrieval**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zKlW4eli4thB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunks = vectorstore.similarity_search(\"What challenge do temporal tracking and forecasting tasks illustrate in machine learning?\",k=2)"
      ],
      "metadata": {
        "id": "nScHuczy4jw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check source document\n",
        "retrieved_chunks[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_t8ImFovUIi",
        "outputId": "0a7e7296-cbbb-4590-bd5f-d0619a25ddec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '2311.06428v2.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What code changes are needed to add both a similarity-score threshold and metadata-based filtering on top of your standard “top­k chunk” retriever in a RAG pipeline?"
      ],
      "metadata": {
        "id": "uJNQ0nP4rFNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### enter code here"
      ],
      "metadata": {
        "id": "RG8y5JT0rGFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 2:** What steps are required to plug a sparse retriever into your RAG workflow, replacing the default dense retriever?"
      ],
      "metadata": {
        "id": "rcO5dPD8qPzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "### enter code here"
      ],
      "metadata": {
        "id": "-LcoVBA-qPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Is there a quick way to evaluate your retrieval results? Hint: Use Metadata"
      ],
      "metadata": {
        "id": "yXAzR8P76gi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## enter code here"
      ],
      "metadata": {
        "id": "fpUOyrUd6uKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Gradio App**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "With your RAG pipeline in place, you’re all set to start chatting with your LLM-powered assistant!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5l3HIFuJrXNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(question):\n",
        "  ####swap your retriever here #####\n",
        "  chunks = vectorstore.similarity_search(question,k=2)\n",
        "  # put the retreived chunks into a context block\n",
        "  joined_chunks = [\"\".join(chunk.page_content) for chunk in chunks]\n",
        "  # reformat them into one Markdown block\n",
        "  context = \"\\n\\n---\\n\\n\".join(joined_chunks)\n",
        "  return context"
      ],
      "metadata": {
        "id": "BfPyZoma0owJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def rag_chat(question: str):\n",
        "    # 1) get context\n",
        "    context = retrieve(question)\n",
        "    # 2) generate answer\n",
        "    answer = rag_generate(context,question)\n",
        "    # return both to the UI\n",
        "    return context, answer\n",
        "# ── 3) Build and launch the app ──\n",
        "iface = gr.Interface(\n",
        "    fn=rag_chat,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask anything…\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Retrieved Context\"),\n",
        "        gr.Textbox(label=\"Answer\")\n",
        "    ],\n",
        "    title=\"Simple RAG Demo\",\n",
        "    description=\"Enter a question, see the retrieved context, and the LLM's answer.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ],
      "metadata": {
        "id": "LMNuoacsuhI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Advance Section**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zTo7KT-Qznzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1 Hybrid Retriever**"
      ],
      "metadata": {
        "id": "AR-p6wFXzxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "## code here"
      ],
      "metadata": {
        "id": "VookXs5qzzrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Which combination method does this ensemble/hybrid retriever use?"
      ],
      "metadata": {
        "id": "_MdS_W1Az91R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.2 Cross-Encoder Reranker**\n",
        "\n",
        "In this section, we’ve provided the code for a cross-encoder reranker. Feel free to explore it and try out different models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e3pApnkB1AsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# load reranker model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
        "model = model.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "def cross_encoder_rerank(question: str, doc: str) -> float:\n",
        "    pairs = [[question, doc]]\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(\n",
        "            pairs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "        ).to(model.device)\n",
        "        scores = model(**inputs).logits.view(-1).float()\n",
        "    return scores.item()\n"
      ],
      "metadata": {
        "id": "v4h84Opl1VMy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Plug the reranker into your current RAG pipeline. Is the reranker’s result better than the initial retrieval result?"
      ],
      "metadata": {
        "id": "M578Y3vA2ByK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## code here"
      ],
      "metadata": {
        "id": "fIhWSlX66bTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "https://aws.amazon.com/compare/the-difference-between-gpus-cpus/"
      ],
      "metadata": {
        "id": "KnC0bb0ilzGu"
      }
    }
  ]
}