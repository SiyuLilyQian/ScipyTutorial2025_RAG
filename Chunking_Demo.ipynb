{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b604cdc7-34a2-488b-8f3e-18d89fb4e864",
      "metadata": {
        "id": "b604cdc7-34a2-488b-8f3e-18d89fb4e864"
      },
      "outputs": [],
      "source": [
        "#!pip install sentence_transformers\n",
        "#!pip install langchain_community\n",
        "#!pip install pypdf\n",
        "#!pip install langchain-experimental\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    TokenTextSplitter\n",
        ")\n",
        "import os\n",
        "import nltk\n",
        "import pypdf\n",
        "from nltk import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae43fed-2bea-415f-b6b7-ac5e2bf85ce6",
      "metadata": {
        "id": "8ae43fed-2bea-415f-b6b7-ac5e2bf85ce6"
      },
      "source": [
        "# Chunking Methods for RAG Pipelines\n",
        "---\n",
        "This notebook demonstrates several methods to split text documents into useful chunks for retrieval-augmented generation (RAG) using LangChain and related libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5489fb2b-2965-4881-8c8b-bdd1623138ca",
      "metadata": {
        "id": "5489fb2b-2965-4881-8c8b-bdd1623138ca"
      },
      "source": [
        "## 1. Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a2e62e-6660-4fbc-a810-4541d46e4fd7",
      "metadata": {
        "id": "81a2e62e-6660-4fbc-a810-4541d46e4fd7"
      },
      "outputs": [],
      "source": [
        "sample_text = \"\"\"# Introduction\n",
        "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation.\n",
        "\n",
        "## What is Chunking?\n",
        "Chunking is the process of dividing text into manageable segments.\n",
        "- Character-based chunking splits purely by length.\n",
        "- Recursive chunking uses natural text breaks, like paragraphs or sentences.\n",
        "- Semantic chunking finds topic changes.\n",
        "\n",
        "Here is a list:\n",
        "1. Fast and easy: character-based.\n",
        "2. Natural: recursive or markdown-based.\n",
        "3. Context-aware: semantic.\n",
        "\n",
        "## An Example with a Superlongword\n",
        "Sometimes, data includes strange artifacts like:\n",
        "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
        "\n",
        "## Topic Change: Semantic Matters\n",
        "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
        "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
        "\n",
        "In summary, choose your chunker based on your data and task! \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144cc0ed-21eb-472a-8dfa-e308254f93f7",
      "metadata": {
        "id": "144cc0ed-21eb-472a-8dfa-e308254f93f7"
      },
      "source": [
        "## 2. Different Chunking Methods\n",
        "### 2.1 Character Splitter\n",
        "Splits text into fixed-length character chunks (with optional overlap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2139a5-41bf-4761-939f-e3e356fc282c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c2139a5-41bf-4761-939f-e3e356fc282c",
        "outputId": "908cc4e8-fc1b-46ee-f95b-ec1a5b356c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks (character splitter): 6\n"
          ]
        }
      ],
      "source": [
        "char_splitter = CharacterTextSplitter(\n",
        "    separator=\"\",\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "char_chunks = char_splitter.split_text(sample_text)\n",
        "print(f\"Total chunks (character splitter): {len(char_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1966db6-2e9c-4c48-8c63-7459c72266aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1966db6-2e9c-4c48-8c63-7459c72266aa",
        "outputId": "19bed645-b6d8-4720-cd36-14fdd11e2c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 (length: 200):\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and gene\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 2 (length: 200):\n",
            "ration.\n",
            "\n",
            "## What is Chunking?\n",
            "Chunking is the process of dividing text into manageable segments. \n",
            "- Character-based chunking splits purely by length.\n",
            "- Recursive chunking uses natural text breaks, lik\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 3 (length: 200):\n",
            "e paragraphs or sentences.\n",
            "- Semantic chunking finds topic changes.\n",
            "\n",
            "Here is a list:\n",
            "1. Fast and easy: character-based.\n",
            "2. Natural: recursive or markdown-based.\n",
            "3. Context-aware: semantic.\n",
            "\n",
            "## An Exam\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 4 (length: 200):\n",
            "ple with a Superlongword\n",
            "Sometimes, data includes strange artifacts like: \n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "\n",
            "## T\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 5 (length: 200):\n",
            "opic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.  \n",
            "Semantic chunking is especially useful when there\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 6 (length: 152):\n",
            "are clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
            "\n",
            "In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(char_chunks, 1):\n",
        "    print(f\"Chunk {i} (length: {len(chunk)}):\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f550ff-fb3c-4e56-a45d-2a70e09561ca",
      "metadata": {
        "id": "90f550ff-fb3c-4e56-a45d-2a70e09561ca"
      },
      "source": [
        "### 2.2 Recursive Splitter\n",
        "Attempts to split text by different logical separators (e.g., paragraphs, sentences) for more \"natural\" chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96bf8ddd-df18-4e0a-9059-ba4af373a4ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96bf8ddd-df18-4e0a-9059-ba4af373a4ee",
        "outputId": "82c1065f-d87b-482b-eafc-bd72b1455a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks (recursive splitter): 10\n"
          ]
        }
      ],
      "source": [
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n",
        ")\n",
        "recursive_chunks = recursive_splitter.split_text(sample_text)\n",
        "print(f\"Total chunks (recursive splitter): {len(recursive_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb4bec2-98f6-434b-8c2a-2484f15d4ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afb4bec2-98f6-434b-8c2a-2484f15d4ac3",
        "outputId": "6ce55b13-b49f-4b96-eb48-a49569b4db9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 (length: 14):\n",
            "# Introduction\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 2 (length: 192):\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 3 (length: 140):\n",
            "## What is Chunking?\n",
            "Chunking is the process of dividing text into manageable segments. \n",
            "- Character-based chunking splits purely by length.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 4 (length: 117):\n",
            "- Recursive chunking uses natural text breaks, like paragraphs or sentences.\n",
            "- Semantic chunking finds topic changes.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 5 (length: 119):\n",
            "Here is a list:\n",
            "1. Fast and easy: character-based.\n",
            "2. Natural: recursive or markdown-based.\n",
            "3. Context-aware: semantic.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 6 (length: 83):\n",
            "## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like:\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 7 (length: 119):\n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 8 (length: 152):\n",
            "## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 9 (length: 140):\n",
            "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 10 (length: 60):\n",
            "In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(recursive_chunks, 1):\n",
        "    print(f\"Chunk {i} (length: {len(chunk)}):\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64dfd0b0-c653-47f0-a0b3-2befefa99e92",
      "metadata": {
        "id": "64dfd0b0-c653-47f0-a0b3-2befefa99e92"
      },
      "source": [
        "### 2.3 Semantic Chunking\n",
        "Chunking based on **semantic similarity**â€”splitting where the topic shifts rather than at fixed lengths.  \n",
        "We'll use `sentence-transformers` to embed sentences and split at points with high semantic \"distance\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "2Bo1o5wVVs5o"
      },
      "id": "2Bo1o5wVVs5o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ef175e-c4e1-4344-82f4-00b25631cb04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ef175e-c4e1-4344-82f4-00b25631cb04",
        "outputId": "b4e2555c-0484-466d-bc31-91b81c9b7e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 2\n",
            "\n",
            "Chunk 1 (length: 484):\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking! This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation. ## What is Chunking? Chunking is the process of dividing text into manageable segments. - Character-based chunking splits purely by length. - Recursive chunking uses natural text breaks, like paragraphs or sentences. - Semantic chunking finds topic changes. Here is a list:\n",
            "1.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 2 (length: 661):\n",
            "Fast and easy: character-based. 2. Natural: recursive or markdown-based. 3. Context-aware: semantic. ## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like: \n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters. ## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter. Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading. In summary, choose your chunker based on your data and task! \n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "emantic_chunker = SemanticChunker(embedding_model, breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=70)\n",
        "semantic_chunks = semantic_chunker.split_text(sample_text)\n",
        "\n",
        "print(f\"Total chunks: {len(semantic_chunks)}\\n\")\n",
        "for i, chunk in enumerate(semantic_chunks, 1):\n",
        "    print(f\"Chunk {i} (length: {len(chunk)}):\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Semantic Chunking with Chunk Size Controlled\n",
        "Using the same logic, but\n",
        "\n",
        "\n",
        "*   Merge small chunks to the previous one\n",
        "*   Set up the max chunk size\n",
        "\n"
      ],
      "metadata": {
        "id": "MFfrPz4JV7Hk"
      },
      "id": "MFfrPz4JV7Hk"
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "-nPn_FbcV6yh"
      },
      "id": "-nPn_FbcV6yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunking(text, max_chunk_size=300, min_chunk_size=100, sim_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Splits text into semantic chunks, then merges small chunks (<min_chunk_size) to the previous,\n",
        "    and ensures no chunk exceeds max_chunk_size.\n",
        "    \"\"\"\n",
        "    model = embedder\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = sentences[0]\n",
        "    current_length = len(sentences[0])\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        # Calculate similarity to previous sentence\n",
        "        similarity = float(util.cos_sim(sentence_embeddings[i], sentence_embeddings[i-1]))\n",
        "        # If semantic similarity is low OR adding would exceed max chunk size, start new chunk\n",
        "        if similarity < sim_threshold or current_length + len(sentences[i]) > max_chunk_size:\n",
        "            # Merge to previous chunk if too small\n",
        "            if len(current_chunk) < min_chunk_size and chunks:\n",
        "                chunks[-1] += \" \" + current_chunk\n",
        "            else:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = sentences[i]\n",
        "            current_length = len(sentences[i])\n",
        "        else:\n",
        "            current_chunk += \" \" + sentences[i]\n",
        "            current_length += len(sentences[i])\n",
        "    # Append the last chunk\n",
        "    if len(current_chunk) < min_chunk_size and chunks:\n",
        "        chunks[-1] += \" \" + current_chunk\n",
        "    else:\n",
        "        chunks.append(current_chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "5fjUJmZSWM_u"
      },
      "id": "5fjUJmZSWM_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunks_2 = semantic_chunking(sample_text, max_chunk_size=300, min_chunk_size=100, sim_threshold=0.7)\n",
        "print(f\"Total chunks: {len(sem_chunks)}\\n\")\n",
        "for i, chunk in enumerate(sem_chunks, 1):\n",
        "    print(f\"Chunk {i} (length: {len(chunk)}):\")\n",
        "    print(\"-\" * 40)\n",
        "    print(chunk)\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO7GILiKWSr2",
        "outputId": "f1ffb99a-383d-41f6-b5a1-69e8e12eca01"
      },
      "id": "zO7GILiKWSr2",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 5\n",
            "\n",
            "Chunk 1 (length: 52):\n",
            "----------------------------------------\n",
            "# Introduction\n",
            "Welcome to our demo on text chunking!\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 2 (length: 532):\n",
            "----------------------------------------\n",
            "This section introduces chunking methods and explains why splitting long documents into smaller pieces is critical for efficient retrieval and generation. ## What is Chunking? Chunking is the process of dividing text into manageable segments. - Character-based chunking splits purely by length. - Recursive chunking uses natural text breaks, like paragraphs or sentences. - Semantic chunking finds topic changes. Here is a list:\n",
            "1. Fast and easy: character-based. 2. Natural: recursive or markdown-based. 3. Context-aware: semantic.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 3 (length: 204):\n",
            "----------------------------------------\n",
            "## An Example with a Superlongword\n",
            "Sometimes, data includes strange artifacts like: \n",
            "ThisIsASingleUnbreakableSupercalifragilisticexpialidociousWordThatExceedsTheChunkSizeLimitAndCausesTroubleForSplitters.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 4 (length: 152):\n",
            "----------------------------------------\n",
            "## Topic Change: Semantic Matters\n",
            "Chunkers that consider **meaning** will split here, as the topic shifts from chunking methods to why semantics matter.\n",
            "----------------------------------------\n",
            "\n",
            "Chunk 5 (length: 201):\n",
            "----------------------------------------\n",
            "Semantic chunking is especially useful when there are clear boundaries in ideas or narrative flow, even if there's no line break or heading. In summary, choose your chunker based on your data and task!\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "anaconda-ai-2024.04-py310",
      "language": "python",
      "name": "conda-env-anaconda-ai-2024.04-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}